# Project Phases: Detailed Task Lists

Detailed implementation plans for each project. See `ROADMAP.md` for the overall philosophy and status.

---

## LP1: Finish the Calendar Tool

### Why This First
You're one step away from a working tool. The psychological reward of seeing transit events appear in your actual calendar is the fuel for everything else.

### Step 1: Real API Integration âœ“

**Tasks:**
- [x] Add Routes API key to `config.json`
- [x] Implement real HTTP call in `transit_calculator.py` (replace stub)
- [x] Test with real travel times
- [x] Handle edge cases (API errors, missing routes)
- [x] Add `--test` CLI for standalone testing
- [x] Add `'stay:'` to trip detection keywords

**Conceptual learning:**
- REST API integration patterns
- Error handling and graceful degradation
- The difference between "it works in stub mode" and "it works for real"

**Definition of Done:** `python add_transit.py` shows accurate travel times from Google Routes API. âœ“ DONE

### Step 1.5: Transit Event Logic Refinements âœ“

**Tasks:**
- [x] Skip creating transit events if duration < 10 minutes
- [x] Skip creating transit events if they would overlap with existing transit events
- [x] Flip trip detection default (process all days; add `--detect-trips` to enable skipping)

**Definition of Done:** Running `--execute` doesn't create redundant or trivially short transit events. âœ“ DONE

### Step 1.6: Dynamic "Home" Based on Stay Events âœ“

**Goal:** On travel days, use the location from "Stay" events as the home location instead of the fixed config address. This makes transit work correctly when traveling.

**Tasks:**
- [x] Extend existing stay detection (reuse `STAY_KEYWORDS`) to also extract the location
- [x] On-demand lookup: `get_stay_location_for_night()` finds Stay event covering a given night
- [x] `get_home_for_transit()` returns prior night's stay (morning) or tonight's stay (evening)
- [x] For transit TO first event: use prior night's Stay location as origin
- [x] For transit HOME after last event: use that night's Stay location as destination
- [x] If no Stay event exists for a date, fall back to config home address
- [x] Add error handling (try/except around Routes API calls)
- [x] Add sanity check: skip transits > 3 hours

**Conceptual learning:**
- State management across days (tracking where you "are" vs where you're "going")
- Graceful fallback patterns
- Error handling for external API failures

**Definition of Done:** Running on a travel week correctly uses hotel/stay locations as the "home" for each day's transit calculations. âœ“ DONE

### Step 1.7: AI-Assisted Location for Stay Events (Future)

**Goal:** Use an AI service to guess the location for Stay events that don't have a location field set.

**Tasks:**
- [ ] Detect Stay events with no location field
- [ ] Use event summary/description to infer location (e.g., "STAY: with Yinne" â†’ look up Yinne's address)
- [ ] Optionally prompt user to confirm/correct inferred location

**Note:** Low priority. For now, ensure Stay events have location fields set manually.

### Step 1.8: Traffic-Aware Routing âœ“

**Goal:** Use departure time and traffic models for more accurate travel time estimates.

**Tasks:**
- [x] Pass `departureTime` to Routes API for traffic predictions
- [x] Query both BEST_GUESS and PESSIMISTIC traffic models for driving routes
- [x] Blend results (75% pessimistic weight) when difference > 25%
- [x] Add traffic note to event description when blended
- [x] Include blending metadata in output

**Conceptual learning:**
- Traffic-aware routing with the Routes API
- Handling API parameters for predictive estimates
- Conservative estimation strategies (blending pessimistic/optimistic)

**Definition of Done:** Transit events use traffic-aware estimates and show when duration was extended due to traffic. âœ“ DONE

### Step 2: Execute Mode âœ“

**Tasks:**
- [x] Change OAuth scope from `calendar.readonly` to `calendar`
- [x] Delete `token.json`, re-authenticate
- [x] Implement `insert_transit_events()` function
- [x] Add safeguards (`--force` flag for confirmation prompt)

**Definition of Done:** `python add_transit.py --execute` creates real transit events in your Google Calendar. âœ“ DONE

### Public Output: GitHub + LinkedIn

**GitHub:**
- Clean up repo, ensure README explains usage
- Remove any hardcoded personal details
- Add MIT license (already done)
- Push to public repo

**LinkedIn post (~200 words):**
```
Theme: "I built a tool that solves a real problem I had"
- The pain point (manually adding transit time to calendar)
- What I built (Python script using Google Calendar + Routes APIs)
- What I learned (OAuth flows, API integration, phased development)
- Link to repo
```

This breaks the seal on "showing work in public" with low stakesâ€”it's a small tool, not a startup pitch.

---

## LP4: Calendar Tool â†’ Chrome Extension

*This phase comes after LP3 (Chatbot basics). By then you'll have more TypeScript/JS exposure.*

### Why This Phase
The Calendar Transit Tool is usefulâ€”but only you can use it. Shipping it as a Chrome extension builds the "ship a product" muscle and teaches cloud deployment.

### Architecture Decision

| Option | Pros | Cons | Skills Learned |
|--------|------|------|----------------|
| **TypeScript rewrite** | Pure client-side, no hosting | More rewrite work | TS, browser APIs |
| **Python API backend** | Reuses existing code | Needs hosting | **Cloud VMs, Docker, deployment** |

**Current path:** TypeScript (pure client-side). The Python backend path remains an option if the current architecture proves unsatisfactoryâ€”it would teach cloud deployment skills.

**If Python backend (future option):**
- [ ] Wrap in FastAPI
- [ ] Dockerize
- [ ] Deploy to cloud VM (DigitalOcean, Fly.io, Railway, etc.)
- [ ] Set up domain/HTTPS

**Conceptual learning:**
- Cloud deployment (VMs, Docker, HTTPS)
- API design for external consumers
- TypeScript fundamentals

### Step 1: MVP Implementation âœ“ (2025-12-27)

**Completed:**
- [x] Extension manifest v3 setup with Bun bundler
- [x] OAuth flow via `chrome.identity.launchWebAuthFlow()`
- [x] Background service worker for OAuth (persists when popup closes)
- [x] Calendar API integration (fetch events, insert transit events)
- [x] Routes API integration (transit times with driving fallback)
- [x] Popup UI with scan â†’ preview â†’ create flow
- [x] Basic filtering (skip video calls, all-day, existing transit)

**Conceptual learning:**
- Chrome extension architecture (manifest v3, service workers)
- OAuth in browser extensions (popup closes during auth â†’ need background worker)
- Product thinking: What's the simplest UX that works?

### Step 2: Feature Parity with CLI

**Tasks to match `add_transit.py` functionality:**
- [ ] Car-only locations (patterns like "22 Lakeview" â†’ always use driving)
- [ ] Traffic-aware routing (BEST_GUESS + PESSIMISTIC blending)
- [ ] Trip detection (flights, hotel stays â†’ dynamic home location)
- [ ] Hold event skipping (colorId 8)
- [ ] Overlap detection between transit events

### Step 3: UX Polish

**Tasks:**
- [ ] Better button copy ("Scan Calendar to Add Transit Events")
- [ ] Driving preference toggle in settings (always drive vs transit fallback)
- [ ] Configurable car-only location patterns in settings
- [ ] Progress indicators during scan
- [ ] Onboarding flow for first-time users
- [ ] Architecture review: Is background worker over-engineered?

### ðŸŽ¯ Recommended Next Session

**Quick wins (30-60 min each):**
1. **Car-only toggle** - Add a simple checkbox in settings: "Always use driving (no transit)". Easiest feature parity item, immediately useful for your 22 Lakeview case.
2. **Better button copy** - Change "Scan Calendar" â†’ "Add Transit Events". Small UX win, 5 minutes.

**Medium effort (1-2 hours):**
3. **Car-only location patterns** - Add a text field in settings for comma-separated patterns. Port the matching logic from CLI's `config.json`.

**If feeling ambitious:**
4. **Architecture review** - Research if `chrome.tabs` API could replace background worker. The current approach works but feels over-engineered.

### Step 4: Publish

**Tasks:**
- [ ] Rotate credentials (API key + OAuth) before any public release
- [ ] Chrome Web Store developer account ($5)
- [ ] Store listing, screenshots, privacy policy
- [ ] Submit for review

**Definition of Done:** Extension live on Chrome Web Store, at least 1 external user.

### Public Output
- Chrome Web Store listing
- LinkedIn post: "I shipped my first Chrome extension"
- (Optional) Blog post on the journey from script â†’ product

### Files
- `calendar-experiments/extension/` - Extension source code
- `calendar-experiments/extension/CLAUDE.md` - Setup instructions
- Branch: `feature/chrome-extension`

---

## LP2: Investment Email Processing (formerly Phase 2)

### Why This Project
You have 30GB of real data (Google Takeout emails) and a real use case (tax prep, portfolio tracking). This is where the "AI agent" roadmap becomes practical.

### Step 1: Email Extraction Pipeline

**Tasks:**
- Parse mbox format from Google Takeout
- Filter to investment-related emails (sender patterns, subject keywords)
- Extract metadata: date, sender, subject, body text
- Store in SQLite `documents` table (schema already exists)

**Conceptual learning:**
- Working with messy real-world data
- Text extraction and cleaning
- The "staging table" pattern for document processing

**Definition of Done:** Investment-related emails loaded into `documents` table with `needs_review = 1`.

### Step 2: Classification and Chunking

**Tasks:**
- Build classifier for document types (capital call, distribution, K-1, etc.)
- Implement chunking strategy for long emails
- Add entity/investment linking (which entity received this?)

**Conceptual learning:**
- Document classification (rule-based first, then consider LLM-assisted)
- Chunking strategies for RAG (why size matters, overlap considerations)
- The value of human-in-the-loop review

**Definition of Done:** Documents classified and chunked, ready for embedding.

### Step 3: RAG Pipeline

**Tasks:**
- Generate embeddings (OpenAI embeddings API or local alternative)
- Store in vector database (Chroma for local, simple setup)
- Build query interface: "What distributions did I receive from X in 2024?"
- Test retrieval quality

**Conceptual learning:**
- **Embeddings:** Text â†’ numbers that capture meaning
- **Vector search:** Finding similar documents by meaning, not keywords
- **RAG pattern:** Retrieve relevant context â†’ stuff into prompt â†’ generate answer

**Definition of Done:** Natural language queries return relevant documents from your investment emails.

### Public Output: Blog Post

This project is meatier and deserves a longer writeup:
- Problem: Tracking private market investments across entities
- Architecture: Email â†’ SQLite â†’ Vector DB â†’ Query interface
- Lessons: What worked, what was harder than expected
- Code snippets and architecture diagrams

**Where to publish:** Your own blog (if you have one), or a LinkedIn article, or dev.to/Medium.

---

## LP3: Agent Wrapper (Chatbot Rebuild)

### Why This Phase
Once RAG is working, wrapping it in an "agent" is relatively straightforward. The reference material in `chatbot-rebuild/` shows the patternâ€”now we build our own to truly understand it.

### Step 1: Basic Agent Loop

**Tasks:**
- Build the agentic while-loop (reference: `chatbot-rebuild/social-manager-agent-unpacked.ts` lines 314-347)
- Define tools: `search_investments(query)`, `get_transactions(investment_id)`, `summarize_document(doc_id)`
- Connect to Claude or GPT-4 via API
- Handle tool calls and responses

**Conceptual learning:**
- The agentic loop pattern (reference material in `chatbot-rebuild/`)
- Tool definition and parameter design
- Prompt engineering for reliable tool use

**Definition of Done:** Ask "What's my total exposure to Fund X?" and get a coherent answer with cited sources.

### Step 2: Memory and Session Management

**Tasks:**
- Add conversation memory (sliding window pattern from reference)
- Session persistence (save/resume conversations)
- Export conversation summaries

**Definition of Done:** Multi-turn conversations work naturally, context persists across questions.

### Public Output: GitHub + Demo Video

- **GitHub:** Investment tracker as open-source tool
- **Demo video (optional):** 2-minute Loom showing natural language queries
- **LinkedIn:** Post about building your first AI agent with real use case

---

## Notes on Tooling Evolution

As projects grow in complexity (Chrome extensions, full-stack apps), consider upgrading development tools:

| Tool | Best For | When to Use |
|------|----------|-------------|
| **Claude Code CLI** | Scripts, single-file tools, quick iterations | LP1-LP3 (current phase) |
| **Cursor / AI IDE** | Multi-file projects, TypeScript, debugging | LP4+ (Chrome extension, larger apps) |

**Don't upgrade prematurely**â€”the current tools work fine for LP1-LP3. But LP4 (Chrome extension with TypeScript) may be the right time to try Cursor or a similar AI-assisted IDE.

The goal isn't to use the fanciest tools. It's to use the simplest tools that get the job done, and upgrade when the complexity genuinely warrants it.
